#### 配置名为app_log的代理，用于采集kafka的 _log_app 主题数据，写入hbase

# 指定代理的组件名称
app_log.sources = r1
app_log.sinks = k1
app_log.channels = c1

# ==============================配置sources组件========
# sources组件类型
app_log.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
app_log.sources.r1.batchSize = 10
app_log.sources.r1.kafka.bootstrap.servers = XCloud170:9092,XCloud171:9092,XCloud172:9092,XCloud174:9092,XCloud175:9092
app_log.sources.r1.topic = _log_app
# 组件要绑定的通道
app_log.sources.r1.channels = c1
app_log.sources.r1.consumer.timeout.ms = 1000
app_log.sources.r1.kafka.consumer.group.id = num43
app_log.sources.r1.kafka.consumer.auto.offset.reset = earliest

# =============================配置sink组件======
# sink组件类型
app_log.sinks.k1.type = hbase2
# HBase中要写入的表的名称
app_log.sinks.k1.table = xz:app_log
# process不要返回backoff，让循环加快
app_log.sinks.k1.backoffEnabled = false
# batchsize 减小到50，缺省100
app_log.sinks.k1.batchSize = 50
# HBase中要写入的列簇
app_log.sinks.k1.columnFamily = logInfo
# 自定义的序列化工具
app_log.sinks.k1.serializer = org.apache.flume.sink.hbase2.RegexHBase2EventSerializer
# 日志拆分提取的正则表达式
app_log.sinks.k1.serializer.regex = (\\S+\\d{13,}_(\\S+)_(\\d+)#\\S+) (.+)
# app_log.sinks.k1.serializer.regex = (.+)
# 指定列簇下的列
app_log.sinks.k1.serializer.colNames = ROW_KEY,host,pid,body
# 指定rowKey的序号
app_log.sinks.k1.serializer.rowKeyIndex = 0
# 组件要绑定的通道
app_log.sinks.k1.channel = c1

# =============================配置channel组件====
# channel 组件的类型
app_log.channels.c1.type = memory
# 将没有数据时的take等待时间减小到0.2秒，缺省是3秒
app_log.channels.c1.keep-alive = 200
app_log.channels.c1.keep-alive-timeunit = ms
# 容量，通道中存储的最大事件数
app_log.channels.c1.capacity = 10000
# 每次交易，channel从source获取，或汇给sink的最大事件数
app_log.channels.c1.transactionCapacity = 10000 
app_log.channels.c1.byteCapacityBufferPercentage = 20
# app_log.channels.c1.byteCapacity = 800000